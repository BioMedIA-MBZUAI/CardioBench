# CardioBench ðŸ«€
**Do Echocardiography Foundation Models Generalize Beyond the Lab?**

> CardioBench is a standardized benchmark that unifies **8 public echocardiography datasets** spanning **4 regression** and **5 classification** tasks, evaluating cardiac-specific, biomedical, and general-purpose foundation models under **zero-shot, probing, and alignment** protocols.

[Preprint](https://arxiv.org/abs/2510.00520) Â· [Datasets](#datasets--tasks) Â· [Getting Started](#getting-started) Â· [Evaluation](#running-evaluations) Â· [Prediction Formats](#prediction-formats) Â· [Citation](#citation)

<p align="center">
  <img src="figs/cardiobench.png" alt="CardioBench overview" width="80%">
</p>

## Datasets & Tasks

- [EchoNet-Dynamic](https://echonet.github.io/dynamic/)  
- [EchoNet-Pediatric](https://echonet.github.io/pediatric/)  
- [EchoNet-LVH](https://echonet.github.io/lvh/)  
- [CAMUS](https://www.creatis.insa-lyon.fr/Challenge/camus/)  
- [TMED-2](https://tmed.cs.tufts.edu/tmed_v2.html)  
- [HMC-QU](https://www.kaggle.com/datasets/aysendegerli/hmcqu-dataset/data)  
- [SegRWMA (Regional Wall Motion Abnormality)](https://www.kaggle.com/datasets/xiaoweixumedicalai/regional-wall-motion-abnormality-echo)  
- [CardiacNet (Abnormal Cardiac Echo Videos)](https://www.kaggle.com/datasets/xiaoweixumedicalai/abnormcardiacechovideos)  

## Repository Layout
- `data/` â€“ split CSVs generated by the workflow.
- `docs/downloads.md` â€“ where and how to obtain each dataset.
- `evaluation/` â€“ per-dataset evaluation scripts, config, sample predictions, and helpers.
- `src/` â€“ training / probing utilities and baseline model code.

## Getting Started
### 1. Download Data
Follow the per-dataset instructions in [`docs/downloads.md`](docs/downloads.md).  

### 2. Configure Paths
Edit `evaluation/config.py` to point to:
- Your prediction folders.
- Desired output directories.

The config file also holds shared constants such as bootstrap count (`B`), seed, view labels, and evaluation split (`SPLIT="test"` by default).

## Running Evaluations
Each dataset/task has a standalone script:
```bash
python evaluation/<script>.py
# Example:
python evaluation/camus.py
```

Outputs (metrics CSVs, per-class accuracy, prediction histograms, plots, etc.) are written to the directories configured in `evaluation/config.py`.

Helpful tips:
- Ensure prediction IDs (`patient_id`, `HashedFileName`, `FileName`, etc.) match the split CSVs exactly.
- View classifiers must include `prob_<VIEW>` columns for every entry in `VIEW_CLASS_NAMES` plus `view_pred`.

## Prediction Formats
Use the CSVs inside `evaluation/example_predictions/` as templates.
- View models can omit `prob_Other`; the evaluator derives it as `1 - sum(prob_<known view>)`.
- Multi-target datasets (e.g., EchoNet-LVH) expect one CSV per measurement, named `<TARGET>_pred`.
- When in doubt, mirror the filenames inside `evaluation/example_predictions/<DATASET>/`.

## Baselines & Protocols
- `src/` contains code for zero-shot similarity, language-aligned prompting, and linear probes on top of frozen encoders.

## Contributing
Contributions are welcome! Please open an issue or pull request for:
1. Evaluation tasks.
2. Bug fixes within the existing scripts.
3. Documentation or visualization improvements.

When submitting predictions or scripts, ensure you do **not** upload raw patient dataâ€”only derived metrics.

## Citation
If CardioBench is useful for your work, please cite:

```bibtex
@article{taratynova2025cardiobench,
  title={CardioBench: Do Echocardiography Foundation Models Generalize Beyond the Lab?},
  author={Taratynova, Darya and Aly, Ahmed and Saeed, Numan and Yaqub, Mohammad},
  journal={arXiv preprint arXiv:2510.00520},
  year={2025}
}
```

Preprint: [arXiv:2510.00520](https://arxiv.org/abs/2510.00520)
